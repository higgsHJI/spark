// Licensed to Cloudera, Inc. under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  Cloudera, Inc. licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
{
  "name" : "SPARK2_ON_YARN",
  "label" : "Spark 2",
  "description" : "Apache Spark is an open source cluster computing system. This service runs Spark 2 as an application on YARN. <span class=\"error\">Before adding this service, ensure that you have installed the Spark2 binaries, which are not included in CDH.</span>",
  "version" : "2.1.0.cloudera1",
  "compatibility" : { "cdhVersion" : { "min" : "5.7.0" } },
  "runAs" : {
    "user" : "spark",
    "group" : "spark",
    "principal" : "spark"
  },
  "inExpressWizard" : true,
  "icon" : "images/icon.png",
  "parcel" : {
    "repoUrl" : "http://archive.cloudera.com/spark2/parcels/latest/",
    "requiredTags" : ["spark2", "cdh"],
    "optionalTags" : ["spark-plugin", "spark2-plugin"]
  },
  "serviceDependencies" : [
    {
      "name" : "YARN",
      "required" : "true"
    },
    {
      "name" : "HIVE",
      "required" : "false"
    }
  ],
  "dependencyExtensions" : [
    {
     "extensionId" : "yarnAuxService",
     "name" : "spark_shuffle",
     "className" : "org.apache.spark.network.yarn.YarnShuffleService",
     "type" : "classAndConfigs",
     "configs" : [
       {
         "key" : "spark.shuffle.service.port",
         "value" : "${spark_shuffle_service_port}"
       },
       {
         "key" : "spark.authenticate",
         "value" : "${spark_authenticate}"
       }
     ]
    }
  ],
  "hdfsDirs" : [
    {
      "name" : "CreateSparkUserDirCommand",
      "label" : "Create Spark User Dir",
      "description" : "Creates the Spark user directory in HDFS.",
      "directoryDescription" : "Spark HDFS user directory",
      "path" : "/user/${principal}",
      "permissions" : "0751"
    },
    {
      "name" : "CreateSparkHistoryDirCommand",
      "label" : "Create Spark History Log Dir",
      "description" : "Creates the directory in HDFS where application history will be stored.",
      "directoryDescription" : "Spark Application History directory",
      "path" : "${spark_history_log_dir}",
      "permissions" : "1777"
    }
  ],
  "serviceInit" : {
    "preStartSteps" : [
      {
        "commandName" : "CreateSparkUserDirCommand"
      },
      {
        "commandName" : "CreateSparkHistoryDirCommand"
      }
    ]
  },
  "parameters" : [
    {
      "name" : "spark_history_log_dir",
      "label" : "Spark History Location (HDFS)",
      "description" : "The location of Spark application history logs in HDFS. Changing this value will not move existing logs to the new location.",
      "configName" : "spark.eventLog.dir",
      "default" : "/user/spark/spark2ApplicationHistory",
      "type" : "path",
      "pathType" : "serviceSpecific",
      "required" : "true"
    },
    {
      "name" : "spark_shuffle_service_port",
      "label" : "Spark Shuffle Service Port",
      "description" : "The port the Spark Shuffle Service listens for fetch requests. If using SPARK_ON_YARN service (i.e. Spark 1 based service), ensure that value of this property is the same in both services.",
      "configName" : "spark.shuffle.service.port",
      "default" : 7337,
      "type" : "port",
      "required" : "true"
    },
    {
      "name" : "spark_authenticate",
      "label" : "Spark Authentication",
      "description" : "Enable whether the Spark communication protocols do authentication using a shared secret. If using SPARK_ON_YARN service (i.e. Spark 1 based service), ensure that value of this property is the same in both services.",
      "configName" : "spark.authenticate",
      "required" : "true",
      "type" : "boolean",
      "default" : false
    }
  ],
  "rolesWithExternalLinks" : ["SPARK2_YARN_HISTORY_SERVER"],
  "roles" : [
    {
      "name" : "SPARK2_YARN_HISTORY_SERVER",
      "label" : "History Server",
      "pluralLabel" : "History Servers",
      "jvmBased": true,
      "startRunner" : {
        "program" : "scripts/control.sh",
        "args" : [ "start_history_server" ],
        "environmentVariables" : {
          "HISTORY_LOG_DIR" : "${spark_history_log_dir}",
          "SPARK_DAEMON_MEMORY" : "${history_server_max_heapsize}"
        }
      },
      "kerberosPrincipals" : [
        {
          "name" : "SPARK_PRINCIPAL",
          "primary" : "${principal}",
          "instance" : "${host}"
        }
      ],
      "externalLink" : {
        "name" : "history_server_web_ui",
        "label" : "History Server Web UI",
        "url" : "http://${host}:${history_server_web_port}"
      },
      "topology" : { "minInstances" : 1, "maxInstances" : 1 },
      "logging" : {
        "configFilename" : "spark2-conf/log4j.properties",
        "dir" : "/var/log/spark2",
        "filename" : "spark2-history-server-${host}.log",
        "modifiable" : true,
        "loggingType" : "log4j"
      },
      "parameters" : [
        {
          "name" : "history_server_web_port",
          "label" : "History Server WebUI Port",
          "configName" : "spark.history.ui.port",
          "description" : "The port of the history server WebUI",
          "required" : "true",
          "type" : "port",
          "default" : 18089
        },
        {
          "name" : "history_server_retained_apps",
          "label" : "Retained App Count",
          "configName" : "spark.history.retainedApplications",
          "description" : "Max number of application UIs to keep in the History Server's memory. All applications will still be available, but may take longer to load if they're not in memory.",
          "required" : "false",
          "type" : "long",
          "min" : 1,
          "default" : 50
        },
        {
          "name" : "history_server_fs_poll_interval",
          "label" : "HDFS Polling Interval",
          "configName" : "spark.history.fs.update.interval.seconds",
          "description" : "How often to poll HDFS for new applications.",
          "required" : "false",
          "type" : "long",
          "unit" : "seconds",
          "default" : 10
        },
        {
          "name" : "history_server_max_heapsize",
          "label" : "Java Heap Size of History Server in Bytes",
          "description" : "Maximum size for the Java process heap memory. Passed to Java -Xmx. Measured in bytes.",
          "required" : "true",
          "type" : "memory",
          "unit" : "bytes",
          "min" : 67108864,
          "default" : 536870912
        },
        {
          "name" : "event_log_cleaner_enabled",
          "label" : "Enable Event Log Cleaner",
          "description" : "Specifies whether the History Server should periodically clean up event logs from storage.",
          "configName" : "spark.history.fs.cleaner.enabled",
          "required" : "false",
          "type" : "boolean",
          "default" : true
        },
        {
          "name" : "event_log_cleaner_interval",
          "label" : "Event Log Cleaner Interval",
          "description" : "How often the History Server will clean up event log files.",
          "configName" : "spark.history.fs.cleaner.interval",
          "required" : "false",
          "type" : "long",
          "unit" : "seconds",
          "default" : 86400
        },
        {
          "name" : "event_log_cleaner_max_age",
          "label" : "Maximum Event Log Age",
          "description" : "Specifies the maximum age of the event logs.",
          "configName" : "spark.history.fs.cleaner.maxAge",
          "required" : "false",
          "type" : "long",
          "unit" : "seconds",
          "default" : 604800
        }
      ],
      "configWriter" : {
        "generators" : [
          {
            "filename" : "spark2-conf/spark-history-server.conf",
            "configFormat" : "properties",
            "includedParams" : [
              "history_server_web_port",
              "history_server_retained_apps",
              "history_server_fs_poll_interval",
              "event_log_cleaner_enabled",
              "event_log_cleaner_interval",
              "event_log_cleaner_max_age"
            ],
            "additionalConfigs" : [
              {
                "key" : "spark.port.maxRetries",
                "value" : "0"
              }
            ]
          }
        ],
        "auxConfigGenerators" : [
          {
            "filename" : "spark2-conf/spark-env.sh",
            "sourceFilename" : "aux/client/spark-env.sh"
          },
          {
            "filename" : "meta/version",
            "sourceFilename" : "meta/version"
          }
        ]
      }
    }
  ],
  "gateway" : {
    "alternatives" : {
      "name" : "spark2-conf",
      "priority" : 51,
      "linkRoot" : "/etc/spark2"
    },
    "parameters" : [
      {
        "name" : "spark_history_enabled",
        "label" : "Enable History",
        "description" : "Write Spark application history logs to HDFS.",
        "configName" : "spark.eventLog.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_deploy_mode",
        "label" : "Default Application Deploy Mode",
        "description" : "Which deploy mode to use by default. Can be overridden by users when launching applications.",
        "required" : "false",
        "type" : "string_enum",
        "validValues" : [ "client", "cluster" ],
        "default" : "client"
      },
      {
        "name" : "spark_data_serializer",
        "label" : "Spark Data Serializer",
        "description" : "Name of class implementing org.apache.spark.serializer.Serializer to use in Spark applications.",
        "configName" : "spark.serializer",
        "default" : "org.apache.spark.serializer.KryoSerializer",
        "type" : "string",
        "required" : "true"
      },
      {
        "name" : "spark_shuffle_service_enabled",
        "label" : "Enable Shuffle Service",
        "description" : "Enables the external shuffle service. The external shuffle service preserves shuffle files written by executors so that the executors can be deallocated without losing work. Must be enabled if Enable Dynamic Allocation is enabled.  Recommended and enabled by default.",
        "configName" : "spark.shuffle.service.enabled",
        "required" : "true",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_python_path",
        "label" : "Extra Python Path",
        "description" : "Python library paths to add to PySpark applications.",
        "required" : "false",
        "type" : "path_array",
        "pathType" : "serviceSpecific",
        "separator" : ":",
        "default" : [ ]
      },
      {
        "name" : "spark_dynamic_allocation_enabled",
        "label" : "Enable Dynamic Allocation",
        "description" : "Enable dynamic allocation of executors in Spark applications.",
        "configName" : "spark.dynamicAllocation.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_dynamic_allocation_initial_executors",
        "label" : "Initial Executor Count",
        "description" : "When dynamic allocation is enabled, number of executors to allocate when the application starts. By default, this is the same value as the minimum number of executors.",
        "configName" : "spark.dynamicAllocation.initialExecutors",
        "required" : "false",
        "type" : "long",
        "min" : 0
      },
      {
        "name" : "spark_dynamic_allocation_min_executors",
        "label" : "Minimum Executor Count",
        "description" : "When dynamic allocation is enabled, minimum number of executors to keep alive while the application is running.",
        "configName" : "spark.dynamicAllocation.minExecutors",
        "required" : "false",
        "type" : "long",
        "min" : 0,
        "default" : 0
      },
      {
        "name" : "spark_dynamic_allocation_max_executors",
        "label" : "Maximum Executor Count",
        "description" : "When dynamic allocation is enabled, maximum number of executors to allocate. By default, Spark relies on YARN to control the maximum number of executors for the application.",
        "configName" : "spark.dynamicAllocation.maxExecutors",
        "required" : "false",
        "type" : "long",
        "min" : 1
      },
      {
        "name" : "spark_dynamic_allocation_idle_timeout",
        "label" : "Executor Idle Timeout",
        "description" : "When dynamic allocation is enabled, time after which idle executors will be stopped.",
        "configName" : "spark.dynamicAllocation.executorIdleTimeout",
        "required" : "false",
        "type" : "long",
        "unit" : "seconds",
        "min" : 0,
        "default" : 60
      },
      {
        "name" : "spark_dynamic_allocation_cached_idle_timeout",
        "label" : "Caching Executor Idle Timeout",
        "description" : "When dynamic allocation is enabled, time after which idle executors with cached RDDs blocks will be stopped. By default, they're never stopped.",
        "configName" : "spark.dynamicAllocation.cachedExecutorIdleTimeout",
        "required" : "false",
        "type" : "long",
        "unit" : "seconds",
        "min" : 0
      },
      {
        "name" : "spark_dynamic_allocation_scheduler_backlog_timeout",
        "label" : "Scheduler Backlog Timeout",
        "description" : "When dynamic allocation is enabled, timeout before requesting new executors when there are backlogged tasks.",
        "configName" : "spark.dynamicAllocation.schedulerBacklogTimeout",
        "required" : "false",
        "type" : "long",
        "unit" : "seconds",
        "min" : 0,
        "default" : 1
      },
      {
        "name" : "spark_dynamic_allocation_sustained_scheduler_backlog_timeout",
        "label" : "Sustained Scheduler Backlog Timeout",
        "description" : "When dynamic allocation is enabled, timeout before requesting new executors after the initial backlog timeout has already expired. By default this is the same value as the initial backlog timeout.",
        "configName" : "spark.dynamicAllocation.sustainedSchedulerBacklogTimeout",
        "required" : "false",
        "type" : "long",
        "unit" : "seconds",
        "min" : 0
      },
      {
        "name" : "spark_gateway_shell_logging_threshold",
        "label" : "Shell Logging Threshold",
        "description" : "The minimum log level for the Spark shell.",
        "required" : "true",
        "type" : "string_enum",
        "validValues" : [ "TRACE", "DEBUG", "INFO", "WARN", "ERROR",  "FATAL" ],
        "default" : "WARN"
      },
      {
        "name" : "spark_gateway_ui_kill_enabled",
        "label" : "Enable Kill From UI",
        "description" : "Whether to allow users to kill running stages from the Spark Web UI.",
        "configName" : "spark.ui.killEnabled",
        "required" : "true",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_kafka_version",
        "label" : "Default Kafka Version",
        "description" : "Default Kafka library version to add to Spark applications. This can be overridden by setting the SPARK_KAFKA_VERSION environment variable when launching Spark applications.",
        "required" : "true",
        "type" : "string_enum",
        "validValues" : [ "0.9", "0.10", "None" ],
        "default" : "0.9"
      }
    ],
    "scriptRunner" : {
      "program" : "scripts/control.sh",
      "args" : [ "client" ],
      "environmentVariables" : {
        "DEPLOY_MODE" : "${spark_deploy_mode}",
        "PYTHON_PATH" : "${spark_python_path}",
        "DEFAULT_SPARK_KAFKA_VERSION" : "${spark_kafka_version}"
      }
    },
    "logging" : {
      "configFilename" : "spark2-conf/log4j.properties",
      "loggingType" : "log4j",
      "additionalConfigs" : [
        { "key" : "shell.log.level", "value" : "${spark_gateway_shell_logging_threshold}" },
        { "key" : "log4j.logger.org.eclipse.jetty", "value" : "WARN" },
        { "key" : "log4j.logger.org.spark-project.jetty", "value" : "WARN" },
        { "key" : "log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle", "value" : "ERROR" },
        { "key" : "log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper", "value" : "INFO" },
        { "key" : "log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter", "value" : "INFO" },
        { "key" : "log4j.logger.org.apache.parquet", "value" : "ERROR" },
        { "key" : "log4j.logger.parquet", "value" : "ERROR" },
        { "key" : "log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler", "value" : "FATAL" },
        { "key" : "log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry", "value" : "ERROR" }
      ]
    },
    "configWriter" : {
      "generators" : [
        {
          "filename" : "spark2-conf/spark-defaults.conf",
          "configFormat" : "properties",
          "includedParams" : [
            "spark_authenticate",
            "spark_history_enabled",
            "spark_history_log_dir",
            "spark_data_serializer",
            "spark_shuffle_service_enabled",
            "spark_shuffle_service_port",
            "spark_dynamic_allocation_enabled",
            "spark_dynamic_allocation_initial_executors",
            "spark_dynamic_allocation_min_executors",
            "spark_dynamic_allocation_max_executors",
            "spark_dynamic_allocation_idle_timeout",
            "spark_dynamic_allocation_cached_idle_timeout",
            "spark_dynamic_allocation_scheduler_backlog_timeout",
            "spark_dynamic_allocation_sustained_scheduler_backlog_timeout",
            "spark_gateway_ui_kill_enabled"
          ]
        }
      ],
      "auxConfigGenerators" : [
        {
          "filename" : "spark2-conf/spark-env.sh",
          "sourceFilename" : "aux/client/spark-env.sh"
        },
        {
          "filename" : "meta/version",
          "sourceFilename" : "meta/version"
        }
      ],
      "peerConfigGenerators" : [
        {
          "filename" : "spark2-conf/history2.properties",
          "params" : ["history_server_web_port"],
          "roleName" : "SPARK2_YARN_HISTORY_SERVER"
        }
      ]
    }
  }
}


